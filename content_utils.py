"""
Content Utilities for HelloComp Category Template Generator
=============================================================

PodpÅ¯rnÃ© funkce pro zpracovÃ¡nÃ­ SEO obsahu.
"""

import re
from typing import List, Dict, Tuple
from bs4 import BeautifulSoup


def clean_html(html_content: str) -> str:
    """
    VyÄistÃ­ HTML od nepotÅ™ebnÃ½ch tagÅ¯ a zachovÃ¡ pouze obsah
    
    Args:
        html_content: HTML string
        
    Returns:
        VyÄiÅ¡tÄ›nÃ½ text
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    return soup.get_text(separator=' ', strip=True)


def extract_links(content: str) -> List[Dict[str, str]]:
    """
    Extrahuje vÅ¡echny odkazy z obsahu
    
    Args:
        content: Markdown nebo HTML obsah
        
    Returns:
        Seznam slovnÃ­kÅ¯ s odkazy (text, url)
    """
    links = []
    
    # Markdown links [text](url)
    md_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
    for match in re.finditer(md_pattern, content):
        links.append({
            'text': match.group(1),
            'url': match.group(2)
        })
    
    # HTML links
    if '<a' in content:
        soup = BeautifulSoup(content, 'html.parser')
        for a_tag in soup.find_all('a'):
            href = a_tag.get('href', '')
            text = a_tag.get_text(strip=True)
            if href:
                links.append({
                    'text': text,
                    'url': href
                })
    
    return links


def count_words(text: str) -> int:
    """
    SpoÄÃ­tÃ¡ poÄet slov v textu
    
    Args:
        text: Text k analÃ½ze
        
    Returns:
        PoÄet slov
    """
    # OdstranÄ›nÃ­ HTML tagÅ¯
    clean_text = re.sub(r'<[^>]+>', '', text)
    # OdstranÄ›nÃ­ Markdown odkazÅ¯
    clean_text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', clean_text)
    # SpoÄÃ­tÃ¡nÃ­ slov
    words = clean_text.split()
    return len([w for w in words if w.strip()])


def extract_keywords(text: str, top_n: int = 10) -> List[Tuple[str, int]]:
    """
    Extrahuje nejÄastÄ›jÅ¡Ã­ klÃ­ÄovÃ¡ slova z textu
    
    Args:
        text: Text k analÃ½ze
        top_n: PoÄet top slov k vrÃ¡cenÃ­
        
    Returns:
        Seznam tuple (slovo, Äetnost)
    """
    # VyÄistit text
    clean_text = re.sub(r'<[^>]+>', '', text.lower())
    clean_text = re.sub(r'[^\w\s]', ' ', clean_text)
    
    # Stopwords (ÄeskÃ¡)
    stopwords = {
        'a', 'i', 'o', 'u', 'v', 'z', 's', 'k', 'na', 'po', 'do', 'od', 'ze', 'se',
        'je', 'jsou', 'jsem', 'jsi', 'jsme', 'jste', 'byl', 'byla', 'bylo', 'byli',
        'by', 'aby', 'kdyby', 'pro', 'jako', 'ale', 'nebo', 'Å¾e', 'aby', 'kdyÅ¾',
        'tak', 'uÅ¾', 'jen', 'uÅ¾', 'co', 'to', 'ta', 'ten', 'ti', 'ty', 'kterÃ©', 'kterÃ½'
    }
    
    words = clean_text.split()
    word_freq = {}
    
    for word in words:
        if len(word) > 2 and word not in stopwords:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # SeÅ™adit podle Äetnosti
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return sorted_words[:top_n]


def format_table(data: List[List[str]], headers: List[str]) -> str:
    """
    FormÃ¡tuje data do Markdown tabulky
    
    Args:
        data: Å˜Ã¡dky dat
        headers: HlaviÄky tabulky
        
    Returns:
        Markdown tabulka
    """
    # Zjistit maximÃ¡lnÃ­ Å¡Ã­Å™ky sloupcÅ¯
    col_widths = [len(h) for h in headers]
    for row in data:
        for i, cell in enumerate(row):
            col_widths[i] = max(col_widths[i], len(str(cell)))
    
    # Sestavit tabulku
    lines = []
    
    # HlaviÄka
    header_line = '| ' + ' | '.join(h.ljust(col_widths[i]) for i, h in enumerate(headers)) + ' |'
    lines.append(header_line)
    
    # OddÄ›lovaÄ
    separator = '|' + '|'.join('-' * (w + 2) for w in col_widths) + '|'
    lines.append(separator)
    
    # Data
    for row in data:
        row_line = '| ' + ' | '.join(str(cell).ljust(col_widths[i]) for i, cell in enumerate(row)) + ' |'
        lines.append(row_line)
    
    return '\n'.join(lines)


def split_into_sections(text: str, max_words_per_section: int = 200) -> List[str]:
    """
    RozdÄ›lÃ­ dlouhÃ½ text na menÅ¡Ã­ sekce
    
    Args:
        text: Text k rozdÄ›lenÃ­
        max_words_per_section: MaximÃ¡lnÃ­ poÄet slov na sekci
        
    Returns:
        Seznam sekcÃ­
    """
    paragraphs = text.split('\n\n')
    sections = []
    current_section = []
    current_word_count = 0
    
    for para in paragraphs:
        para_words = len(para.split())
        
        if current_word_count + para_words > max_words_per_section and current_section:
            sections.append('\n\n'.join(current_section))
            current_section = [para]
            current_word_count = para_words
        else:
            current_section.append(para)
            current_word_count += para_words
    
    if current_section:
        sections.append('\n\n'.join(current_section))
    
    return sections


def generate_toc(content: str) -> str:
    """
    Generuje obsah (Table of Contents) z H2 nadpisÅ¯
    
    Args:
        content: Markdown obsah
        
    Returns:
        Markdown seznam odkazÅ¯
    """
    h2_pattern = re.compile(r'^##\s+(.+?)$', re.MULTILINE)
    headings = h2_pattern.findall(content)
    
    toc_lines = ['## Obsah', '']
    for i, heading in enumerate(headings, 1):
        # VytvoÅ™enÃ­ anchor linku
        anchor = heading.lower().replace(' ', '-')
        anchor = re.sub(r'[^\w\-]', '', anchor)
        toc_lines.append(f'{i}. [{heading}](#{anchor})')
    
    return '\n'.join(toc_lines)


def validate_internal_links(content: str, valid_paths: List[str]) -> List[Dict[str, str]]:
    """
    Validuje internÃ­ odkazy v obsahu
    
    Args:
        content: Obsah s odkazy
        valid_paths: Seznam platnÃ½ch internÃ­ch cest
        
    Returns:
        Seznam problÃ©movÃ½ch odkazÅ¯
    """
    links = extract_links(content)
    invalid_links = []
    
    for link in links:
        url = link['url']
        # InternÃ­ odkazy zaÄÃ­najÃ­ / nebo obsahujÃ­ hellocomp.cz
        if url.startswith('/') or 'hellocomp.cz' in url:
            # Extrakce cesty
            path = url.split('hellocomp.cz')[-1] if 'hellocomp.cz' in url else url
            path = path.split('#')[0].split('?')[0]  # OdstranÄ›nÃ­ anchoru a query
            
            if path not in valid_paths:
                invalid_links.append({
                    'text': link['text'],
                    'url': url,
                    'path': path
                })
    
    return invalid_links


def add_emoji_markers(content: str) -> str:
    """
    PÅ™idÃ¡ emoji markery k dÅ¯leÅ¾itÃ½m ÄÃ¡stem
    
    Args:
        content: Obsah bez emoji
        
    Returns:
        Obsah s emoji markery
    """
    # PÅ™idat âš¡ k vÃ½konnostnÃ­m metrikÃ¡m
    content = re.sub(r'\b(\d+\s*FPS)\b', r'âš¡ \1', content)
    content = re.sub(r'\b(\d+\s*GB)\b', r'ğŸ’¾ \1', content)
    content = re.sub(r'\b(\d+K)\b', r'ğŸ® \1', content)
    
    # PÅ™idat âœ… k vÃ½hodÃ¡m
    content = re.sub(r'^-\s+([^-])', r'- âœ… \1', content, flags=re.MULTILINE)
    
    return content


def optimize_for_seo(text: str, primary_keyword: str, secondary_keywords: List[str] = None) -> str:
    """
    Optimalizuje text pro SEO pÅ™idÃ¡nÃ­m klÃ­ÄovÃ½ch slov
    
    Args:
        text: PÅ¯vodnÃ­ text
        primary_keyword: PrimÃ¡rnÃ­ klÃ­ÄovÃ© slovo
        secondary_keywords: Seznam sekundÃ¡rnÃ­ch klÃ­ÄovÃ½ch slov
        
    Returns:
        SEO-optimalizovanÃ½ text
    """
    if secondary_keywords is None:
        secondary_keywords = []
    
    # Ensure primary keyword appears in first paragraph
    paragraphs = text.split('\n\n')
    if paragraphs and primary_keyword.lower() not in paragraphs[0].lower():
        # Try to naturally include it
        first_para = paragraphs[0]
        # Simple insertion at the end of first sentence
        sentences = first_para.split('.')
        if sentences:
            sentences[0] = sentences[0] + f' s {primary_keyword}'
            paragraphs[0] = '.'.join(sentences)
    
    return '\n\n'.join(paragraphs)
